{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "data = pd.read_csv('MINST_train.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>pixel0</th>\n",
       "      <th>pixel1</th>\n",
       "      <th>pixel2</th>\n",
       "      <th>pixel3</th>\n",
       "      <th>pixel4</th>\n",
       "      <th>pixel5</th>\n",
       "      <th>pixel6</th>\n",
       "      <th>pixel7</th>\n",
       "      <th>pixel8</th>\n",
       "      <th>...</th>\n",
       "      <th>pixel774</th>\n",
       "      <th>pixel775</th>\n",
       "      <th>pixel776</th>\n",
       "      <th>pixel777</th>\n",
       "      <th>pixel778</th>\n",
       "      <th>pixel779</th>\n",
       "      <th>pixel780</th>\n",
       "      <th>pixel781</th>\n",
       "      <th>pixel782</th>\n",
       "      <th>pixel783</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows Ã— 785 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   label  pixel0  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  \\\n",
       "0      1       0       0       0       0       0       0       0       0   \n",
       "1      0       0       0       0       0       0       0       0       0   \n",
       "2      1       0       0       0       0       0       0       0       0   \n",
       "\n",
       "   pixel8  ...  pixel774  pixel775  pixel776  pixel777  pixel778  pixel779  \\\n",
       "0       0  ...         0         0         0         0         0         0   \n",
       "1       0  ...         0         0         0         0         0         0   \n",
       "2       0  ...         0         0         0         0         0         0   \n",
       "\n",
       "   pixel780  pixel781  pixel782  pixel783  \n",
       "0         0         0         0         0  \n",
       "1         0         0         0         0  \n",
       "2         0         0         0         0  \n",
       "\n",
       "[3 rows x 785 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        1\n",
       "1        0\n",
       "2        1\n",
       "3        4\n",
       "4        0\n",
       "        ..\n",
       "41995    0\n",
       "41996    1\n",
       "41997    7\n",
       "41998    6\n",
       "41999    9\n",
       "Name: label, Length: 42000, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.pop(\"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.array(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X Shape (Rows, Cols) (784, 42000)\n",
      "Y Shape (Rows, Cols) (42000,)\n"
     ]
    }
   ],
   "source": [
    "# Tranposing flips columns/rows.\n",
    "# You may need to transpose if you're doing a dot product of two matrices.\n",
    "# Why? Because dot product of two matrices does columns in one against rows in another.\n",
    "# So if the column is length 4 in m1, and row is length 3 in m2, it is invalid as non-matching lengths.\n",
    "\n",
    "# Assign X & Y, and divide by 255 to scale to 0-1 and have no overflow error when doing exponential operations.\n",
    "    # Divide by 255 as there are 256 possible pixel values (0->255)\n",
    "\n",
    "# We transpose the data to match the shape the weights matrices will be in:\n",
    "#   Neurons:\n",
    "#   [ neuron 1      weight1, weight2, weight3 ]\n",
    "#   [ neuron 2      weight1, weight2, weight3 ]\n",
    "#   [ neuron 3      weight1, weight2, weight3 ]\n",
    "#   [ neuron 10     weight1, weight2, weight3 ]\n",
    "#\n",
    "#   Pixels:\n",
    "#   [ pixel1        example1, example2, example3 ]\n",
    "#   [ pixel2        example1, example2, example3 ]\n",
    "#   [ pixel3        example1, example2, example3 ]\n",
    "#   ...\n",
    "#   [ pixel748      example1, example2, example3 ]\n",
    "\n",
    "# Data -> [ pix1, pix2, pix3 .....\n",
    "#         [ example1 pixel 1, example1 pixel2, example1 pixel3 ....\n",
    "#         [ example2 pixel 1, example2 pixel2, example2 pixel3 ....\n",
    "X = data.T / 255\n",
    "\n",
    "# Data.T (Transposed, or \"Flipped\") ->  [ pix1 example1 pixel 1, example2 pixel 1, example3 pixel1, .....\n",
    "#                                       [ pix2 example1 pixel 2, example2 pixel 2, example3 pixel2, .....\n",
    "#                                       [ pix3 example1 pixel 3, example2 pixel 3, example3 pixel3, .....\n",
    "\n",
    "# Before transposing, each column to the right is a new pixel of the same example\n",
    "# After transposing, each column to the right is a new example of the same pixel\n",
    "\n",
    "Y = np.array(pd.read_csv('MINST_train.csv').pop(\"label\")) / 255\n",
    "\n",
    "print(\"X Shape (Rows, Cols)\", X.shape)  # 784 pixels of a 28x28 image down the way, and 42000 image examples along to the right\n",
    "print(\"Y Shape (Rows, Cols)\", Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A neural net is made up of inputs, weights for all inputs for each neuron, bias(es), an activation function for each neuron, & final outputs\n",
    "    # 1. Input features\n",
    "    # 2. Layer(s) of Neurons: A neuron is a set of weights for all inputs, a bias, and an activation function\n",
    "    # 3. Output\n",
    "\n",
    "# Training a neural network is done by propagating inputs through the layers of the network, then going backwards to optimise based on an error value.\n",
    "# You do this repeatedly.\n",
    "\n",
    "# Need weights & biases\n",
    "def initialise_neural_net_parameters(n_features, n_neurons=10):\n",
    "    '''\n",
    "    @returns layer1Weights, layer1Biases, layer2Weights, layer2Biases\n",
    "    \n",
    "    - W1: Each neuron's weights for every input feature (all input features go to each neuron)\n",
    "    - b1: biases for each neuron in the first layer\n",
    "    - W2: Each neuron's weights for each of the previous layer's outputs (all inputs from last layer go to each neuron)\n",
    "    - b2: biases for each in neuron the second layer\n",
    "    '''\n",
    "\n",
    "    layer1Weights = np.random.rand(n_neurons, n_features)  # 10 rows, 784 columns -> weights for all features\n",
    "                                                    # Each row is a neuron, and each neuron has 784 weights\n",
    "                                                    # There are 784 weights per neuron, as each neuron receives input from each pixel in input layer\n",
    "    layer2Weights = np.random.rand(n_neurons, n_neurons)   # 10 rows, 10 columns -> weights for the 10 neurons in 2nd layer\n",
    "                                                    # Each row is a neuron, and each neuron has 10 weights\n",
    "                                                    # There are 10 weights per neuron as each neuron receives input from the 10 neurons in last layer\n",
    "\n",
    "    layer1Biases = np.random.rand(10, 1)  # 10 rows, 1 value per row\n",
    "    layer2Biases = np.random.rand(10, 1)  # 10 rows, 1 value per row\n",
    "\n",
    "    return layer1Weights, layer1Biases, layer2Weights, layer2Biases\n",
    "\n",
    "def ReLU(input):\n",
    "    '''RELU result is 0 if input is < 0, else it is just the same as the input'''\n",
    "    return np.maximum(0, input)\n",
    "\n",
    "def ReLU_derivative(input):\n",
    "    '''Returns slope of line as 1 if greater than 0, else slope of line as 0 if <= 0'''\n",
    "    return input > 0\n",
    "\n",
    "def softmax(input):\n",
    "    '''Converts a vector of numbers into a vector of probabilities based on the total (if 1,1,1, then probability of each is .33)'''\n",
    "    return np.exp(input) / np.sum(np.exp(input))\n",
    "\n",
    "# Need forward propagation to train\n",
    "def forward_propagation(layer1InputWeights, layer1Biases, layer2InputWeights, layer2Biases, X):\n",
    "    '''\n",
    "    Calculate output of each layer\n",
    "    @returns layer1FunctionInput, layer1FunctionOutput, layer2FunctionInput, layer2FunctionOutput\n",
    "    '''\n",
    "    \n",
    "    # Output of a layer is Ï•((X * W) + b)\n",
    "        # Ï• is activation function\n",
    "        # X is matrix of inputs (we will multiply each input by weight)\n",
    "        # W is weights (we will multiply each weight by each input)\n",
    "        # b is bias\n",
    "    \n",
    "    # 1st layer uses pixel input with weights & bias\n",
    "    layer1FunctionInput = layer1InputWeights.dot(X) + layer1Biases\n",
    "    layer1FunctionOutput = ReLU(layer1FunctionInput)       # functions are what make a NN linear or non-linear\n",
    "\n",
    "    # 2nd layer uses output from layer 1 as input against its weights & bias\n",
    "    layer2FunctionInput = layer2InputWeights.dot(layer1FunctionOutput) + layer2Biases\n",
    "    layer2FunctionOutput = softmax(layer2FunctionInput)    # functions are what make a NN linear or non-linear\n",
    "\n",
    "    return layer1FunctionInput, layer1FunctionOutput, layer2FunctionInput, layer2FunctionOutput\n",
    "\n",
    "def backward_propagation(L1WB, layer1Output, L2WB, layer2Output, layer2Weights, X, Y):\n",
    "    '''\n",
    "    Figures out how much each part of each layer contributed to the error\n",
    "    @returns l1WErr, l1bErr, l2WErr, l2bErr\n",
    "    '''\n",
    "    \n",
    "    # TODO: 1-hot encoding?\n",
    "\n",
    "    # how much did each part of each layer contribute to error?\n",
    "    layer2Error = layer2Output - Y\n",
    "    layer2WeightError = (1 / Y.size) * layer2Error.dot(layer1Output.T)\n",
    "    layer2BiasError = (1 / Y.size) * np.average(layer2Error)\n",
    "\n",
    "    layer1Error = layer2Weights.T.dot(layer2Error) * ReLU_derivative(layer1Output)\n",
    "    layer1WeightError = (1 / Y.size) * layer1Error.dot(X.T)\n",
    "    layer1BiasError = (1 / Y.size) * np.average(layer1Error)\n",
    "\n",
    "    return layer1WeightError, layer1BiasError, layer2WeightError, layer2BiasError\n",
    "\n",
    "def update_neural_net_parameters(layer1Weights, layer1Biases, layer2Weights, layer2Biases, errL1W, errL1B, errL2W, errL2B, alpha):\n",
    "    '''@returns new W1, b1, W2, b2'''\n",
    "    layer1Weights = layer1Weights - alpha * errL1W\n",
    "    layer1Biases = layer1Biases - alpha * errL1B\n",
    "\n",
    "    layer2Weights = layer2Weights - alpha * errL2W\n",
    "    layer2Biases = layer2Biases - alpha * errL2B\n",
    "\n",
    "    return layer1Weights, layer1Biases, layer2Weights, layer2Biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(L2A):\n",
    "    # Get indices of largest values\n",
    "    return np.argmax(L2A, 0)\n",
    "\n",
    "def get_accuracy(predictions, Y):\n",
    "    return np.sum(predictions == Y) / Y.size\n",
    "\n",
    "def gradient_descent(X, Y, iterations, alpha):\n",
    "    layer1Weights, layer1Biases, layer2Weights, layer2Biases = initialise_neural_net_parameters(n_features=X.shape[0])\n",
    "\n",
    "    for i in range(iterations):\n",
    "        layer1WeightBiasOutput, layer1Output, layer2WeightBiasOutput, layer2Output = forward_propagation(layer1Weights, layer1Biases,\n",
    "                                                                                                            layer2Weights, layer2Biases,\n",
    "                                                                                                            X)\n",
    "        # Go back through layers & figure out error\n",
    "        l1WErr, l1bErr, l2WErr, l2bErr = backward_propagation(layer1WeightBiasOutput, layer1Output, \n",
    "                                                                layer2WeightBiasOutput, layer2Output,\n",
    "                                                                layer2Weights, X, Y)\n",
    "\n",
    "        # Get new weights & biases based on error\n",
    "        layer1Weights, layer1Biases, layer2Weights, layer2Biases = update_neural_net_parameters(layer1Weights, layer1Biases, \n",
    "                                                                                                layer2Weights, layer2Biases, \n",
    "                                                                                                l1WErr, l1bErr, l2WErr, l2bErr, \n",
    "                                                                                                alpha)\n",
    "        \n",
    "        if i % 10 == 0:\n",
    "            print(\"Iteration:\", i)\n",
    "            predictions = get_predictions(layer2WeightBiasOutput)\n",
    "            print(get_accuracy(predictions, Y))\n",
    "    \n",
    "    return layer1Weights, layer1Biases, layer2Weights, layer2Biases    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\Python3.7\\lib\\site-packages\\ipykernel_launcher.py:42: RuntimeWarning: overflow encountered in exp\n",
      "f:\\Python3.7\\lib\\site-packages\\ipykernel_launcher.py:42: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0\n",
      "0.0\n",
      "Iteration: 10\n",
      "0.09838095238095237\n",
      "Iteration: 20\n",
      "0.09838095238095237\n",
      "Iteration: 30\n",
      "0.09838095238095237\n",
      "Iteration: 40\n",
      "0.09838095238095237\n"
     ]
    }
   ],
   "source": [
    "l1Weights, l1Biases, l2Weights, l2Biases = gradient_descent(X, Y, iterations=50, alpha=0.10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "eec67bece78a636467b96713ad9783ecddf94761545077dbecfcf88302223227"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
